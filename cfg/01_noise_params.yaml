model:
  class_path: drumblender.tasks.DrumBlender
  init_args:
    encoder: null
    modal_synth: drumblender.synths.ModalSynth
    modal_autoencoder: null
    transient_synth: null
    transient_autoencoder: null
    noise_synth: synths/noise.yaml
    noise_autoencoder: models/encoder/noise_soundstream.yaml
    noise_autoencoder_accepts_audio: true
    loss_fn: loss/mss.yaml
    transient_parallel: false
    float32_matmul_precision: medium
optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.0001
    weight_decay: 1e-6
lr_scheduler:
  class_path: pytorch_lightning.cli.ReduceLROnPlateau
  init_args:
    monitor: validation/loss_epoch
    factor: 0.5
    patience: 5
    # ### HIGHLIGHT: `min_lr` belongs to ReduceLROnPlateau (scheduler), not AdamW.
    min_lr: 0.00000625
    threshold_mode: rel
    threshold: 0.01
data: data/percussion.yaml
trainer:
  devices: 1
  accelerator: gpu
  # ### HIGHLIGHT: Stop runaway training by default; use scripts to override when needed.
  max_epochs: 70
  # logger:
  #   class_path: pytorch_lightning.loggers.WandbLogger
  #   init_args:
  #     name: noise_params
  #     project: drumblender
  #     save_dir: logs
  #     log_model: true
  #   dict_kwargs:
  #     job_type: train
  #     group: ablation
  #     entity: jordieshier
  callbacks:
    - class_path: pytorch_lightning.callbacks.early_stopping.EarlyStopping
      init_args:
        monitor: validation/loss_epoch
        patience: 9
        min_delta: 0.01
        check_finite: true
        # ### HIGHLIGHT: Count patience at epoch boundaries even when validation runs multiple times per epoch.
        check_on_train_epoch_end: true
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        # ### HIGHLIGHT: Save checkpoints under repository-local `ckpt/` for easy reuse.
        dirpath: ckpt
        filename: epoch{epoch:03d}-step{step:08d}
        # ### HIGHLIGHT: Avoid duplicated names like `epochepoch=...`.
        auto_insert_metric_name: false
        monitor: validation/loss_epoch
        mode: min
        save_top_k: 10
        save_last: true
        save_on_train_epoch_end: true
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
    - class_path: drumblender.callbacks.LogAudioCallback
      init_args:
        on_train: true
        on_val: true
        on_test: false
        save_audio_sr: 48000
        n_batches: 1
        log_on_epoch_end: true
        max_audio_samples: 8
    # - class_path: drumblender.callbacks.CleanWandbCacheCallback
    #   init_args:
    #     every_n_epochs: 2
    #     max_size_in_gb: 2
    # - class_path: pytorch_lightning.callbacks.BatchSizeFinder
    #   init_args:
    #     mode: binsearch
    #     steps_per_trial: 10
    #     init_val: 2
    #     max_trials: 25
    #     batch_arg_name: batch_size
seed_everything: 20260218
